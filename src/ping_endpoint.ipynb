{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/miniconda3/envs/inference/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def make_openai_request(content: str, model: str = \"string\", temperature: float = 0, max_tokens: int = 0, url: str = \"http://stargate:8000/v1/chat/completions\"):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            { \"role\": \"user\", \"content\": content }\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    headers = { \"Content-Type\": \"application/json\" }\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def process_item(prompt: str):\n",
    "    response = make_openai_request(prompt, \n",
    "                                   model = \"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "                                   max_tokens=2048,\n",
    "                                   temperature=1)\n",
    "    return {'prompt': prompt, 'response': response}\n",
    "\n",
    "def process_dataset(dataset_name: str):\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    train_data = dataset['test']\n",
    "    results = []\n",
    "    # count = 0\n",
    "    for item in tqdm(train_data, desc=\"Processing prompts\"):\n",
    "        # if count == 5:\n",
    "        #     break\n",
    "        result = process_item(item['prompt'])\n",
    "        results.append(result)\n",
    "       \n",
    "        # count += 1\n",
    "        # print(result['response'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 164/164 [33:50<00:00, 12.38s/it]\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = \"verifiers-for-code/humaneval_plan_generation\"\n",
    "results = process_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<thinking>\n",
      "To create an effective action plan for this problem, I need to break down the solution into clear, logical steps that guide the implementation without providing the full code. The plan should cover:\n",
      "1. Understanding the problem requirements\n",
      "2. Defining a helper function to validate a group of parentheses\n",
      "3. Implementing the main function to separate groups of parentheses\n",
      "4. Iterating through the input string and handling opening and closing parentheses\n",
      "5. Returning the list of separated groups\n",
      "</thinking>\n",
      "\n",
      "<plan>\n",
      "Action Plan:\n",
      "1. Define the `separate_paren_groups` function, which takes a string as input.\n",
      "2. Create a helper function `is_valid_group` to validate a group of parentheses:\n",
      "    a. Initialize a stack to store opening parentheses\n",
      "    b. Iterate through the group, adding opening parentheses to the stack and removing closing parentheses\n",
      "    c. Return `True` if the stack is empty (valid group) and `False` otherwise\n",
      "3. Initialize an empty list `groups` to store the separated groups of parentheses.\n",
      "4. Iterate through each character in the input string:\n",
      "    a. If the character is an opening parenthesis, add it to the stack\n",
      "    b. If the character is a closing parenthesis:\n",
      "        i. Check if the stack is empty (no matching opening parenthesis)\n",
      "        ii. If the stack is empty, add the closing parenthesis to the `groups` list\n",
      "        iii. Pop the top element from the stack and remove it from the input string\n",
      "5. Return the `groups` list containing the separated groups of parentheses.\n",
      "\n",
      "Note: Be careful when handling the stack and popping elements to avoid incorrect results.\n",
      "Use appropriate string methods for character manipulation and iteration.\n",
      "</plan>\n"
     ]
    }
   ],
   "source": [
    "print(results[1][\"response\"][\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<thinking>\\nTo design an action plan for this problem, I\\'ll break down the solution into logical steps that guide the implementation without giving away the exact code. The plan should cover the main components of the solution, including the initialization of variables, the loop to compare elements, the calculation of distances, and the final return value. I\\'ll provide clear instructions for each step, using the existing code structure and comments as a reference. The plan should be comprehensive enough to guide the implementation without providing the exact solution.\\n</thinking>\\n\\n<plan>\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n\\n    # Step 1: Initialize a variable to store the closest distance\\n    closest_distance = float(\\'inf\\')\\n\\n    # Step 2: Iterate through each pair of numbers in the list\\n    for i in range(len(numbers)):\\n        for j in range(i + 1, len(numbers)):\\n\\n            # Step 3: Calculate the absolute difference between the pair\\n            distance = abs(numbers[i] - numbers[j])\\n\\n            # Step 4: Update the closest distance if the current distance is smaller\\n            if distance < closest_distance:\\n                closest_distance = distance\\n\\n    # Step 5: Check if the closest distance is less than the threshold\\n    return closest_distance < threshold\\n</plan>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][\"response\"][\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_COL_NAME = \"l3_plans_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['task_id',\n",
       " 'prompt',\n",
       " 'canonical_solution',\n",
       " 'test',\n",
       " 'entry_point',\n",
       " 'sonnet-3.5_gold_plans',\n",
       " 'cleaned_sonnet-3.5_gold_plans',\n",
       " 'generated_phi3_baseline',\n",
       " 'generated_phi3_plan_generation',\n",
       " 'phi3-planner-plans',\n",
       " 'cleaned-phi3-planner-plans',\n",
       " 'self_planning_Phi-3-mini-4k-instruct',\n",
       " 'cleaned-self_planning_Phi-3-mini-4k-instruct',\n",
       " 'phi3-planner-granular',\n",
       " 'cleaned-phi3-planner-granular',\n",
       " 'l3_plans_non_gran',\n",
       " 'cleaned-l3_plans_non_gran',\n",
       " 'l3_plans_new']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "\n",
    "# Extract the 'test' split we worked with\n",
    "test_data = dataset['test']\n",
    "\n",
    "# Create a list of the new column data\n",
    "phi3_plans = [result[\"response\"][\"choices\"][0][\"message\"][\"content\"] for result in results]\n",
    "\n",
    "if NEW_COL_NAME in test_data.column_names:\n",
    "    test_data = test_data.remove_columns(NEW_COL_NAME)\n",
    "\n",
    "# Add the new column to the dataset\n",
    "test_data = test_data.add_column(NEW_COL_NAME, phi3_plans)\n",
    "\n",
    "test_data.column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 164/164 [00:00<00:00, 9306.30 examples/s] \n"
     ]
    }
   ],
   "source": [
    "test_data.save_to_disk(NEW_COL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 40.64ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/verifiers-for-code/humaneval_plan_generation/commit/a2348544b90d5c81741078d84c115f5428b3124c', commit_message='Upload dataset', commit_description='', oid='a2348544b90d5c81741078d84c115f5428b3124c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.push_to_hub(DATASET_NAME, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.04k/1.04k [00:00<00:00, 3.89MB/s]\n",
      "Downloading data: 100%|██████████| 1.17M/1.17M [00:00<00:00, 3.28MB/s]\n",
      "Generating test split: 100%|██████████| 164/164 [00:00<00:00, 11323.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['task_id',\n",
       " 'prompt',\n",
       " 'canonical_solution',\n",
       " 'test',\n",
       " 'entry_point',\n",
       " 'sonnet-3.5_gold_plans',\n",
       " 'cleaned_sonnet-3.5_gold_plans',\n",
       " 'generated_phi3_baseline',\n",
       " 'generated_phi3_plan_generation',\n",
       " 'phi3-planner-plans',\n",
       " 'cleaned-phi3-planner-plans',\n",
       " 'self_planning_Phi-3-mini-4k-instruct',\n",
       " 'cleaned-self_planning_Phi-3-mini-4k-instruct',\n",
       " 'phi3-planner-granular',\n",
       " 'cleaned-phi3-planner-granular',\n",
       " 'l3_plans_non_gran']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
