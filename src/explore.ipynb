{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/miniconda3/envs/inference/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-25 04:17:11,041\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from evalplus.data import get_human_eval_plus\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting Prompts + VLLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"microsoft/Phi-3-mini-4k-instruct\" \n",
    "MODEL_NAME = MODEL.split('/')[-1] # used to decide what to call output dir\n",
    "DATASET = \"verifiers-for-code/humaneval_plan\" \n",
    "OUTPUT_DIR = MODEL_NAME + \"-output\"\n",
    "NUM_GPUS = 1\n",
    "COLUMN_NAME = \"generated_phi3_baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "__MAGIC_SPLITTER__ = \"-[[]]-this-is-really-our-highest-priority-[[]]-\"\n",
    "response = f\"\"\"\n",
    "Below is a self-contained Python script that solves the problem: \n",
    "```python \n",
    "{__MAGIC_SPLITTER__}\n",
    "```\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eplus = get_human_eval_plus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-25 04:17:13 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='microsoft/Phi-3-mini-4k-instruct', speculative_config=None, tokenizer='microsoft/Phi-3-mini-4k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=microsoft/Phi-3-mini-4k-instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/miniconda3/envs/inference/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-25 04:17:13 utils.py:660] Found nccl from library /home/vaibhav/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 06-25 04:17:15 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 06-25 04:17:15 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 06-25 04:17:17 model_runner.py:175] Loading model weights took 7.1183 GB\n",
      "INFO 06-25 04:17:18 gpu_executor.py:114] # GPU blocks: 2565, # CPU blocks: 682\n",
      "INFO 06-25 04:17:22 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-25 04:17:22 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-25 04:17:23 model_runner.py:1017] Graph capturing finished in 1 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=MODEL, \n",
    "          tensor_parallel_size = NUM_GPUS, \n",
    "          enable_prefix_caching=False, \n",
    "          gpu_memory_utilization=0.95, \n",
    "          max_model_len=2048, \n",
    "          trust_remote_code=True,\n",
    "          max_num_seqs = 16)\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0, top_p=0.95, max_tokens = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_none_prompts(prompt):\n",
    "    prompt = f\"Please provide a self-contained Python script that solves the following problem in a markdown code block. Follow the given plan.\\n```\\n{prompt.strip()}\\n```\\n\" \n",
    "    x = tokenizer.apply_chat_template(\n",
    "         [ \n",
    "             {\"role\": \"user\", \"content\": prompt}, \n",
    "             {\"role\": \"assistant\", \"content\": response}, \n",
    "         ], \n",
    "    tokenize=False).split(__MAGIC_SPLITTER__)[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_prompts = []\n",
    "\n",
    "for heval_task in eplus.keys():\n",
    "    pr = eplus[heval_task]['prompt']\n",
    "    pr = create_none_prompts(pr)           \n",
    "    none_prompts.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|user|>\n",
      "Please provide a self-contained Python script that solves the following problem in a markdown code block. Follow the given plan.\n",
      "```\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "```\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "Below is a self-contained Python script that solves the problem: \n",
      "```python \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(none_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vllm_code(px):\n",
    "    outputs = llm.generate(px, sampling_params)\n",
    "    outputs = [x.outputs[0].text for x in outputs]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 164/164 [01:14<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "none_prompts_sol = get_vllm_code(none_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to hf\n",
    "def update_dataset_with_solutions(dataset, new_column_name, solutions):\n",
    "    \n",
    "    dataset = dataset.add_column(new_column_name, solutions)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = update_dataset_with_solutions(dataset, COLUMN_NAME, none_prompts_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLLM Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_code(text):\n",
    "    # Find the index of the triple backticks\n",
    "    index = text.find(\"```\")\n",
    "    \n",
    "    # If the triple backticks are found, slice the string up to that point\n",
    "    if index != -1:\n",
    "        text = text[:index]\n",
    "    \n",
    "    # Split the text into lines\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    # Reverse the list to find the last \"return\" from the end\n",
    "    for i, line in enumerate(reversed(lines)):\n",
    "        if \"return\" in line:\n",
    "            # Find the line number of the last \"return\" statement\n",
    "            last_return_index = len(lines) - i - 1\n",
    "            # Return the text up to and including the last \"return\" line\n",
    "            return '\\n'.join(lines[:last_return_index+1])\n",
    "    \n",
    "    # If no \"return\" is found, or no backticks are found, return the original text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_code_to_eval(dataset_name, code_column_name):\n",
    "#     return dataset_name[code_column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_solutions = [extract_clean_code(code) for code in none_prompts_sol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List\n",
      "\n",
      "def separate_paren_groups(paren_string: str) -> List[str]:\n",
      "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
      "    separate those group into separate strings and return the list of those.\n",
      "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
      "    Ignore any spaces in the input string.\n",
      "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
      "    ['()', '(())', '(()())']\n",
      "    \"\"\"\n",
      "    # Remove spaces from the input string\n",
      "    paren_string = paren_string.replace(\" \", \"\")\n",
      "\n",
      "    # Initialize variables\n",
      "    result = []\n",
      "    current_group = \"\"\n",
      "    open_braces = 0\n",
      "\n",
      "    for char in paren_string:\n",
      "        if char == \"(\":\n",
      "            open_braces += 1\n",
      "            current_group += char\n",
      "        elif char == \")\":\n",
      "            open_braces -= 1\n",
      "            if open_braces == 0:\n",
      "                result.append(current_group)\n",
      "                current_group = \"\"\n",
      "        else:\n",
      "            continue\n",
      "\n",
      "    return result\n"
     ]
    }
   ],
   "source": [
    "print(none_solutions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(none_solutions)):\n",
    "    name = f\"HumanEval_{index}\"\n",
    "    os.makedirs(os.path.join(f\"{OUTPUT_DIR}/none\", name), exist_ok=True)\n",
    "    with open(os.path.join(f\"{OUTPUT_DIR}/none\", name, '0.py'), 'w', encoding='utf-8') as f:\n",
    "        f.write(none_solutions[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_jsonl(solutions, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for i, solution in enumerate(solutions):\n",
    "            json_line = json.dumps({\"task_id\": f\"HumanEval/{i}\", \"completion\": solution})\n",
    "            f.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_jsonl(none_solutions, f\"{OUTPUT_DIR}/none/solutions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing expected output...\n",
      "Expected outputs computed in 18.49s\n",
      "Reading samples...\n",
      "164it [00:00, 403.92it/s]\n",
      "100%|█████████████████████████████████████████| 164/164 [00:07<00:00, 20.90it/s]\n",
      "\u001b[31mhumaneval (base tests)\u001b[0m\n",
      "\u001b[31mpass@1:\t0.628\u001b[0m\n",
      "\u001b[32mhumaneval+ (base + extra tests)\u001b[0m\n",
      "\u001b[32mpass@1:\t0.591\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!evalplus.evaluate --dataset humaneval --samples $OUTPUT_DIR/none "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
