{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/miniconda3/envs/inference/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-26 01:06:01,376\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from evalplus.data import get_human_eval_plus\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting Prompts + VLLM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"microsoft/Phi-3-mini-4k-instruct\" \n",
    "MODEL_NAME = MODEL.split('/')[-1] # used to decide what to call output dir\n",
    "DATASET = \"verifiers-for-code/humaneval_plan_generation\" \n",
    "OUTPUT_DIR = MODEL_NAME + \"-output\"\n",
    "NUM_GPUS = 1\n",
    "COLUMN_NAME = \"generated_phi3_baseline\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "__MAGIC_SPLITTER__ = \"-[[]]-this-is-really-our-highest-priority-[[]]-\"\n",
    "response = f\"\"\"\n",
    "Below is a self-contained Python script that solves the problem: \n",
    "```python \n",
    "{__MAGIC_SPLITTER__}\n",
    "```\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eplus = get_human_eval_plus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-26 01:06:07 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='microsoft/Phi-3-mini-4k-instruct', speculative_config=None, tokenizer='microsoft/Phi-3-mini-4k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=microsoft/Phi-3-mini-4k-instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/miniconda3/envs/inference/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-26 01:06:07 utils.py:660] Found nccl from library /home/vaibhav/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 06-26 01:06:08 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 06-26 01:06:09 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 06-26 01:06:11 model_runner.py:175] Loading model weights took 7.1183 GB\n",
      "INFO 06-26 01:06:12 gpu_executor.py:114] # GPU blocks: 2565, # CPU blocks: 682\n",
      "INFO 06-26 01:06:16 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-26 01:06:16 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-26 01:06:17 model_runner.py:1017] Graph capturing finished in 2 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=MODEL, \n",
    "          tensor_parallel_size = NUM_GPUS, \n",
    "          enable_prefix_caching=False, \n",
    "          gpu_memory_utilization=0.95, \n",
    "          max_model_len=2048, \n",
    "          trust_remote_code=True,\n",
    "          max_num_seqs = 16)\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0, top_p=0.95, max_tokens = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_none_prompts(prompt):\n",
    "    prompt = f\"Please provide a self-contained Python script that solves the following problem in a markdown code block. Follow the given plan.\\n```\\n{prompt.strip()}\\n```\\n\" \n",
    "    x = tokenizer.apply_chat_template(\n",
    "         [ \n",
    "             {\"role\": \"user\", \"content\": prompt}, \n",
    "             {\"role\": \"assistant\", \"content\": response}, \n",
    "         ], \n",
    "    tokenize=False).split(__MAGIC_SPLITTER__)[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none_prompts = []\n",
    "\n",
    "# for heval_task in eplus.keys():\n",
    "#     print(eplus[heval_task].keys())\n",
    "#     pr = eplus[heval_task]['cleaned_sonnet-3.5_gold_plans']\n",
    "#     pr = create_none_prompts(pr)           \n",
    "#     none_prompts.append(pr)\n",
    "\n",
    "none_prompts = []\n",
    "for task in dataset:\n",
    "    none_prompts.append(create_none_prompts(task['cleaned_sonnet-3.5_gold_plans']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|user|>\n",
      "Please provide a self-contained Python script that solves the following problem in a markdown code block. Follow the given plan.\n",
      "```\n",
      "def generate_integers(a, b):\n",
      "    \"\"\"\n",
      "    Given two positive integers a and b, return the even digits between a\n",
      "    and b, in ascending order.\n",
      "\n",
      "    For example:\n",
      "    generate_integers(2, 8) => [2, 4, 6, 8]\n",
      "    generate_integers(8, 2) => [2, 4, 6, 8]\n",
      "    generate_integers(10, 14) => []\n",
      "\n",
      "    Action Plan:\n",
      "    1. Determine the lower and upper bounds:\n",
      "       - Consider that a and b can be in any order\n",
      "       - The lower bound should be the smaller of a and b, but not less than 2\n",
      "       - The upper bound should be the larger of a and b, but not more than 8\n",
      "       - Hint: Use min() and max() functions to handle this logic\n",
      "\n",
      "    2. Generate a list of even integers:\n",
      "       - Create a list comprehension that:\n",
      "         a. Iterates through the range from the lower bound to the upper bound (inclusive)\n",
      "         b. Includes only even numbers in the result\n",
      "       - Hint: Use the modulo operator (%) to check for even numbers\n",
      "\n",
      "    3. Return the resulting list\n",
      "\n",
      "    Remember to handle edge cases where there might be no even digits in the given range.\n",
      "    \"\"\"\n",
      "```\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "Below is a self-contained Python script that solves the problem: \n",
      "```python \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(none_prompts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vllm_code(px):\n",
    "    outputs = llm.generate(px, sampling_params)\n",
    "    outputs = [x.outputs[0].text for x in outputs]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 164/164 [01:23<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "none_prompts_sol = get_vllm_code(none_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # push to hf\n",
    "# def update_dataset_with_solutions(dataset, new_column_name, solutions):\n",
    "    \n",
    "#     dataset = dataset.add_column(new_column_name, solutions)\n",
    "    \n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The table can't have duplicated columns but columns ['generated_phi3_baseline'] are duplicated.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_dataset_with_solutions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCOLUMN_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnone_prompts_sol\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m, in \u001b[0;36mupdate_dataset_with_solutions\u001b[0;34m(dataset, new_column_name, solutions)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_dataset_with_solutions\u001b[39m(dataset, new_column_name, solutions):\n\u001b[0;32m----> 4\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_column_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolutions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/miniconda3/envs/inference/lib/python3.9/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/inference/lib/python3.9/site-packages/datasets/fingerprint.py:482\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/inference/lib/python3.9/site-packages/datasets/arrow_dataset.py:5850\u001b[0m, in \u001b[0;36mDataset.add_column\u001b[0;34m(self, name, column, new_fingerprint)\u001b[0m\n\u001b[1;32m   5823\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add column to Dataset.\u001b[39;00m\n\u001b[1;32m   5824\u001b[0m \n\u001b[1;32m   5825\u001b[0m \u001b[38;5;124;03m<Added version=\"1.7\"/>\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5847\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m   5848\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5849\u001b[0m column_table \u001b[38;5;241m=\u001b[39m InMemoryTable\u001b[38;5;241m.\u001b[39mfrom_pydict({name: column})\n\u001b[0;32m-> 5850\u001b[0m \u001b[43m_check_column_names\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcolumn_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5851\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_indices() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   5852\u001b[0m \u001b[38;5;66;03m# Concatenate tables horizontally\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/inference/lib/python3.9/site-packages/datasets/arrow_dataset.py:654\u001b[0m, in \u001b[0;36m_check_column_names\u001b[0;34m(column_names)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m count \u001b[38;5;129;01min\u001b[39;00m counter\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    653\u001b[0m     duplicated_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m counter \u001b[38;5;28;01mif\u001b[39;00m counter[col] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe table can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have duplicated columns but columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduplicated_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are duplicated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The table can't have duplicated columns but columns ['generated_phi3_baseline'] are duplicated."
     ]
    }
   ],
   "source": [
    "# dataset = update_dataset_with_solutions(dataset, COLUMN_NAME, none_prompts_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.push_to_hub(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLLM Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_code(text):\n",
    "    # Find the index of the triple backticks\n",
    "    index = text.find(\"```\")\n",
    "    \n",
    "    # If the triple backticks are found, slice the string up to that point\n",
    "    if index != -1:\n",
    "        text = text[:index]\n",
    "    \n",
    "    # Split the text into lines\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    # Reverse the list to find the last \"return\" from the end\n",
    "    for i, line in enumerate(reversed(lines)):\n",
    "        if \"return\" in line:\n",
    "            # Find the line number of the last \"return\" statement\n",
    "            last_return_index = len(lines) - i - 1\n",
    "            # Return the text up to and including the last \"return\" line\n",
    "            return '\\n'.join(lines[:last_return_index+1])\n",
    "    \n",
    "    # If no \"return\" is found, or no backticks are found, return the original text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_code_to_eval(dataset_name, code_column_name):\n",
    "#     return dataset_name[code_column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_solutions = [extract_clean_code(code) for code in none_prompts_sol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def generate_integers(a, b):\n",
      "    \"\"\"\n",
      "    Given two positive integers a and b, return the even digits between a\n",
      "    and b, in ascending order.\n",
      "\n",
      "    For example:\n",
      "    generate_integers(2, 8) => [2, 4, 6, 8]\n",
      "    generate_integers(8, 2) => [2, 4, 6, 8]\n",
      "    generate_integers(10, 14) => []\n",
      "\n",
      "    Action Plan:\n",
      "    1. Determine the lower and upper bounds:\n",
      "       - Consider that a and b can be in any order\n",
      "       - The lower bound should be the smaller of a and b, but not less than 2\n",
      "       - The upper bound should be the larger of a and b, but not more than 8\n",
      "       - Hint: Use min() and max() functions to handle this logic\n",
      "\n",
      "    2. Generate a list of even integers:\n",
      "       - Create a list comprehension that:\n",
      "         a. Iterates through the range from the lower bound to the upper bound (inclusive)\n",
      "         b. Includes only even numbers in the result\n",
      "       - Hint: Use the modulo operator (%) to check for even numbers\n",
      "\n",
      "    3. Return the resulting list\n",
      "\n",
      "    Remember to handle edge cases where there might be no even digits in the given range.\n",
      "    \"\"\"\n",
      "    lower_bound = max(2, min(a, b))\n",
      "    upper_bound = min(8, max(a, b))\n",
      "\n",
      "    even_digits = [num for num in range(lower_bound, upper_bound + 1) if num % 2 == 0]\n",
      "\n",
      "    return even_digits\n"
     ]
    }
   ],
   "source": [
    "print(none_solutions[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(none_solutions)):\n",
    "    name = f\"HumanEval_{index}\"\n",
    "    os.makedirs(os.path.join(f\"{OUTPUT_DIR}/gold_plans\", name), exist_ok=True)\n",
    "    with open(os.path.join(f\"{OUTPUT_DIR}/gold_plans\", name, '0.py'), 'w', encoding='utf-8') as f:\n",
    "        f.write(none_solutions[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_jsonl(solutions, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for i, solution in enumerate(solutions):\n",
    "            json_line = json.dumps({\"task_id\": f\"HumanEval/{i}\", \"completion\": solution})\n",
    "            f.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_jsonl(none_solutions, f\"{OUTPUT_DIR}/gold_plans/solutions.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from ground-truth from /home/vaibhav/.cache/evalplus/84f4b93a1270b492e4c54d5212da7a5b.pkl\n",
      "Reading samples...\n",
      "164it [00:00, 450.00it/s]\n",
      "100%|█████████████████████████████████████████| 164/164 [00:08<00:00, 19.52it/s]\n",
      "\u001b[31mhumaneval (base tests)\u001b[0m\n",
      "\u001b[31mpass@1:\t0.841\u001b[0m\n",
      "\u001b[32mhumaneval+ (base + extra tests)\u001b[0m\n",
      "\u001b[32mpass@1:\t0.774\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!evalplus.evaluate --dataset humaneval --samples $OUTPUT_DIR/gold_plans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
